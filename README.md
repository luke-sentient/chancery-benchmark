# chancery-benchmark

Law has long been a domain that has been popular in natural language processing (NLP) applications. While multiple legal datasets exist, none have thus far focused specifically on reasoning tasks, despite the fact that reasoning (ratiocination and the ability to make connections to precedent) is a core part of the practice of the law in the real world. We focus on a specific aspect of the legal landscape by introducing a corporate governance reasoning benchmark **(the CHANCERY benchmark)** to test a model's ability to reason about whether executive/board/shareholder's proposed actions are consistent with corporate governance charters. This benchmark introduces a first-of-its-kind corporate governance reasoning test for language models – modeled after real world corporate governance law. The benchmark consists of a corporate charter (a set of governing covenants) and a proposal for executive action. The model’s task is one of binary classification: reason about whether the action is consistent with the rules contained within the charter. We create the benchmark following established principles of corporate governance - 24 concrete corporate governance principles established in *Gompers et al. (2003)* and 79 real life corporate charters selected to represent diverse industries from a total dataset of 10k real life corporate charters. Evaluations on state-of-the-art reasoning models confirm the difficulty of the benchmark, with models such as Claude 3.7 Sonnet and DeepSeek-R1 achieving 64.5\% and 58.6\%  accuracy respectively. We further conduct an analysis of the types of questions which current reasoning models struggle on, revealing insights into the legal reasoning capabilities of state-of-the-art models.
