{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Install dependencies**"
      ],
      "metadata": {
        "id": "nL_l9PiNpH5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install jsonlines 'smolagents[litellm]'"
      ],
      "metadata": {
        "id": "SiE2-uzVVGWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Input API keys**"
      ],
      "metadata": {
        "id": "ipH6Yb2vner0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"FIREWORKS_API_KEY\"] = \"YOUR-FIREWORKS-API-KEY\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"YOUR-OPENAI-API-KEY\"\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = \"ANTHROPIC-API-KEY\""
      ],
      "metadata": {
        "id": "KXAdLfenWC9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Run Evaluation on LLM of choice**"
      ],
      "metadata": {
        "id": "pP2NJ2oEnd1A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l12trnUySdGj"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import jsonlines\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import time\n",
        "from litellm import completion\n",
        "from litellm.exceptions import RateLimitError\n",
        "\n",
        "def evaluate_chancery(jsonl_path, csv_path, model=\"fireworks_ai/llama-v3p3-70b-instruct\",\n",
        "                                 max_items=502, max_tokens=10, temperature=0.0, max_retries=5):\n",
        "    \"\"\"\n",
        "    Evaluates an LLM on the CHANCERY benchmark by matching the charter id with the\n",
        "    corresponding charter text and querying an LLM via litellm.\n",
        "    Includes handling for rate limits and time estimation.\n",
        "\n",
        "    Args:\n",
        "    - jsonl_path (str): Path to the JSONL file with questions and answers\n",
        "    - csv_path (str): Path to the CSV file with charter text\n",
        "    - model (str): LLM model to use (default: \"fireworks_ai/llama-v3p3-70b-instruct\")\n",
        "    - max_items (int): Maximum number of items to process\n",
        "    - max_tokens (int): Maximum tokens in model response\n",
        "    - temperature (float): Temperature setting for model (0.0 = more deterministic)\n",
        "    - max_retries (int): Maximum number of retry attempts on rate limit errors\n",
        "\n",
        "    Returns:\n",
        "    - tuple: (results dataframe, accuracy percentage)\n",
        "    \"\"\"\n",
        "    # Read the CSV file\n",
        "    print(f\"Loading charter texts from {csv_path}...\")\n",
        "    charter_texts = pd.read_csv(csv_path, on_bad_lines='skip')\n",
        "    print(f\"Loaded {len(charter_texts)} charter entries\")\n",
        "\n",
        "    # Prepare results storage\n",
        "    results = []\n",
        "    correct_predictions = 0\n",
        "    total_processed = 0\n",
        "\n",
        "    print(f\"Using model: {model}\")\n",
        "\n",
        "    # Count total items for progress bar (up to max_items)\n",
        "    with jsonlines.open(jsonl_path) as reader:\n",
        "        total_items = sum(1 for _ in reader if _ is not None)\n",
        "    total_items = min(total_items, max_items)\n",
        "    print(f\"Processing up to {total_items} questions from {jsonl_path}\")\n",
        "\n",
        "    # Read JSONL file\n",
        "    with jsonlines.open(jsonl_path) as reader:\n",
        "        # Initialize tqdm with total count and time estimation\n",
        "        pbar = tqdm(total=total_items, desc=\"Processing Charters\", unit=\"charter\")\n",
        "\n",
        "        for i, item in enumerate(reader):\n",
        "            if i >= max_items:\n",
        "                break\n",
        "\n",
        "            question = item['question']\n",
        "            expected_answer = item['answer']\n",
        "            charter_id = item['charter_id']\n",
        "\n",
        "            # Find matching charter text\n",
        "            matching_text = charter_texts[charter_texts['charter_id'] == charter_id]['text'].values\n",
        "\n",
        "            if len(matching_text) == 0:\n",
        "                print(f\"No matching text found for charter_id: {charter_id}\")\n",
        "                continue\n",
        "\n",
        "            # Create prompt for the model\n",
        "            prompt = f\"\"\"Based on the charter text below, please answer the following question with ONLY 'Yes' or 'No'.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Charter Text: {matching_text[0]}\n",
        "\n",
        "Your response must be EXACTLY ONE WORD: either 'Yes' or 'No'. No additional text or explanation is allowed.\"\"\"\n",
        "\n",
        "            # Query the model with retry logic for rate limits\n",
        "            retry_count = 0\n",
        "            backoff_time = 2  # starting backoff time in seconds\n",
        "\n",
        "            while True:\n",
        "                try:\n",
        "                    response = completion(\n",
        "                        model=model,\n",
        "                        messages=[{\"content\": prompt, \"role\": \"user\"}],\n",
        "                        max_tokens=max_tokens,\n",
        "                        temperature=temperature\n",
        "                    )\n",
        "\n",
        "                    # Extract the model's response\n",
        "                    model_response = response.choices[0].message.content.strip().lower()\n",
        "\n",
        "                    # Extract prediction (just take the first word in case model outputs extra text)\n",
        "                    prediction = model_response.split()[0].capitalize()\n",
        "                    if prediction not in ['Yes', 'No']:\n",
        "                        prediction = 'No'  # Default to No if model doesn't follow instructions\n",
        "\n",
        "                    is_correct = prediction == expected_answer\n",
        "\n",
        "                    if is_correct:\n",
        "                        correct_predictions += 1\n",
        "\n",
        "                    results.append({\n",
        "                        'charter_id': charter_id,\n",
        "                        'question': question,\n",
        "                        'expected_answer': expected_answer,\n",
        "                        'model_response': model_response,\n",
        "                        'prediction': prediction,\n",
        "                        'is_correct': is_correct,\n",
        "                        'token_usage': response.usage,\n",
        "                        'retries': retry_count\n",
        "                    })\n",
        "\n",
        "                    # Break out of the retry loop on success\n",
        "                    break\n",
        "\n",
        "                except RateLimitError as e:\n",
        "                    retry_count += 1\n",
        "                    if retry_count > max_retries:\n",
        "                        print(f\"Failed after {max_retries} retries for charter {charter_id}: {e}\")\n",
        "                        results.append({\n",
        "                            'charter_id': charter_id,\n",
        "                            'question': question,\n",
        "                            'expected_answer': expected_answer,\n",
        "                            'model_response': \"RATE_LIMIT_ERROR\",\n",
        "                            'prediction': \"No\",  # Default to No on failure\n",
        "                            'is_correct': expected_answer == \"No\",\n",
        "                            'token_usage': None,\n",
        "                            'retries': retry_count\n",
        "                        })\n",
        "                        break\n",
        "\n",
        "                    wait_time = backoff_time * (2 ** (retry_count - 1))  # Exponential backoff\n",
        "                    print(f\"Rate limit hit for charter {charter_id}. Retrying in {wait_time} seconds... (Attempt {retry_count}/{max_retries})\")\n",
        "                    time.sleep(wait_time)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing charter {charter_id}: {e}\")\n",
        "                    print(f\"Exception details: {type(e).__name__}: {e}\")\n",
        "\n",
        "                    # Still add the result with error info\n",
        "                    results.append({\n",
        "                        'charter_id': charter_id,\n",
        "                        'question': question,\n",
        "                        'expected_answer': expected_answer,\n",
        "                        'model_response': f\"ERROR: {type(e).__name__}\",\n",
        "                        'prediction': \"No\",  # Default to No on failure\n",
        "                        'is_correct': expected_answer == \"No\",\n",
        "                        'token_usage': None,\n",
        "                        'retries': retry_count\n",
        "                    })\n",
        "                    break\n",
        "\n",
        "            # Update progress bar\n",
        "            total_processed += 1\n",
        "            pbar.update(1)\n",
        "\n",
        "        # Close progress bar\n",
        "        pbar.close()\n",
        "\n",
        "    # Convert results to DataFrame\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = (correct_predictions / total_processed) * 100 if total_processed > 0 else 0\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(f\"\\nResults Summary:\")\n",
        "    print(f\"Model used: {model}\")\n",
        "    print(f\"Processed {total_processed} charters with {correct_predictions} correct predictions\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"Total retries due to rate limits: {results_df['retries'].sum()}\")\n",
        "\n",
        "    return results_df, accuracy\n",
        "\n",
        "def setup_api_keys(fireworks_key=None, openai_key=None, anthropic_key=None):\n",
        "    \"\"\"Set up API keys for different LLM providers\"\"\"\n",
        "    keys_set = []\n",
        "\n",
        "    if fireworks_key:\n",
        "        os.environ[\"FIREWORKS_API_KEY\"] = fireworks_key\n",
        "        keys_set.append(\"Fireworks\")\n",
        "\n",
        "    if openai_key:\n",
        "        os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "        keys_set.append(\"OpenAI\")\n",
        "\n",
        "    if anthropic_key:\n",
        "        os.environ[\"ANTHROPIC_API_KEY\"] = anthropic_key\n",
        "        keys_set.append(\"Anthropic\")\n",
        "\n",
        "    if keys_set:\n",
        "        print(f\"API keys set for: {', '.join(keys_set)}\")\n",
        "    else:\n",
        "        print(\"No API keys provided. Please ensure keys are set in your environment variables.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example configuration - can be modified or moved to command-line arguments\n",
        "    config = {\n",
        "        'jsonl_path': 'CHANCERY benchmark.jsonl',\n",
        "        'csv_path': 'charters.csv',\n",
        "        'model': 'fireworks_ai/llama-v3p3-70b-instruct',  # Can be changed to any LiteLLM supported model\n",
        "        'max_items': 10,  # Set to small number for testing; use larger number for full run\n",
        "        'output_file': 'charter_analysis_results.csv'\n",
        "    }\n",
        "\n",
        "    # Optional: Set API keys if not in environment variables\n",
        "    # setup_api_keys(\n",
        "    #     fireworks_key=\"your_fireworks_key\",\n",
        "    #     openai_key=\"your_openai_key\",\n",
        "    #     anthropic_key=\"your_anthropic_key\"\n",
        "    # )\n",
        "\n",
        "    # Run the analysis\n",
        "    results_df, accuracy = evaluate_chancery(\n",
        "        jsonl_path=config['jsonl_path'],\n",
        "        csv_path=config['csv_path'],\n",
        "        model=config['model'],\n",
        "        max_items=config['max_items']\n",
        "    )\n",
        "\n",
        "    # Save results\n",
        "    results_df.to_csv(config['output_file'], index=False)\n",
        "    print(f\"Results saved to {config['output_file']}\")"
      ]
    }
  ]
}